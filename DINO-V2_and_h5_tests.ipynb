{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cdca41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'tiger, Panthera tigris', 'score': 0.24335609376430511},\n",
       " {'label': 'tiger cat', 'score': 0.24146227538585663},\n",
       " {'label': 'lynx, catamount', 'score': 0.16084855794906616},\n",
       " {'label': 'marmot', 'score': 0.043973542749881744},\n",
       " {'label': 'tabby, tabby cat', 'score': 0.03293466567993164}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HF Example\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"image-classification\",\n",
    "    model=\"facebook/dinov2-small-imagenet1k-1-layer\",\n",
    "    dtype=torch.float16,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8def47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc1c43d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(cls_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90459edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.8935735   0.5611535   0.7221834  ... -1.6112721  -1.7305301\n",
      "  -1.24625   ]\n",
      " [ 0.959288    1.5554346  -0.45038044 ...  0.38674653 -1.9512769\n",
      "   0.13411409]\n",
      " [ 0.90353054  4.137877    1.1482705  ... -3.668721    0.68040836\n",
      "   1.5894419 ]\n",
      " ...\n",
      " [ 1.7816428  -0.3815659   0.29235205 ... -0.32355672  0.05422599\n",
      "   0.9596687 ]\n",
      " [-1.0558149   0.5838056  -1.5675911  ... -1.1514442   0.01467256\n",
      "  -1.4091729 ]\n",
      " [ 0.10558449  1.7960936  -1.2238214  ... -0.98181576  2.1069787\n",
      "  -1.3743852 ]]\n",
      "[[48.284294  4.02083 ]\n",
      " [37.122986 -3.568118]\n",
      " [52.24937   8.912947]\n",
      " ...\n",
      " [44.60789  11.12855 ]\n",
      " [50.534054 -3.564082]\n",
      " [50.379196  4.45012 ]]\n"
     ]
    }
   ],
   "source": [
    "file=hdf5.open_HDF5(\"downloaded_embeddings.h5\")\n",
    "vectors=file[\"vectors\"]\n",
    "vectors=vectors[:] #load into a np array\n",
    "labels=file[\"coordinates\"]\n",
    "labels=labels[:]\n",
    "print(vectors)\n",
    "print(labels)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b9a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinates: shape=(3167055, 2), dtype=float32\n",
      "vectors: shape=(3167055, 768), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import hdf5\n",
    "#Size of hp5\n",
    "print(\"agah\")\n",
    "path=\"/home/adam/source/CLIP/full_dataset_embeddings.h5\"\n",
    "hdf5.get_size(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the clip model\n",
    "\n",
    "\n",
    "import utils\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "importlib.reload(utils)\n",
    "from tqdm import tqdm\n",
    "\n",
    "#for having the right model; print the model if unsurea about the layers\n",
    "dim_fourier_encoding=64 #multiple of 4!!\n",
    "dim_hidden=256\n",
    "dim_emb=128 #this one is actually shared with img embeddings\n",
    "device=\"cuda\"\n",
    "data_path=\"/home/adam/source/CLIP/full_dataset_embeddings.h5\"\n",
    "#dim image layer size: \n",
    "#As of now: linear from 768 to dim_emb. We could also have MLP if non-linearity needed\n",
    "\n",
    "image_encoder=nn.Linear(768,dim_emb).to(device)\n",
    "pos_encoder=utils.Fourier_MLP(original_dim=2, fourier_dim=dim_fourier_encoding, hidden_dim=dim_hidden, output_dim=dim_emb).to(device)\n",
    "\n",
    "model= utils.DoubleNetwork(image_encoder,pos_encoder).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/adam/source/CLIP/save_14_to_17-10-25/model.pt\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "nbr_iter=500000\n",
    "mean_sim, mean_asim, std_sim, std_asim = utils.test_similarity(data_path,model, nbr_samples=2,device=\"cuda\",nbr_iter=nbr_iter,plot=True)\n",
    "print(\"Mean:\", mean_sim, mean_asim)\n",
    "print(\"Std:\", std_sim, std_asim)\n",
    "#for the whole test  set:\n",
    "#Mean: tensor(0.1810, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0827, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "#Std: tensor(0.0645, device='cuda:0', grad_fn=<SqrtBackward0>) tensor(0.0726, device='cuda:0', grad_fn=<SqrtBackward0>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
