{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cdca41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'tiger, Panthera tigris', 'score': 0.24335609376430511},\n",
       " {'label': 'tiger cat', 'score': 0.24146227538585663},\n",
       " {'label': 'lynx, catamount', 'score': 0.16084855794906616},\n",
       " {'label': 'marmot', 'score': 0.043973542749881744},\n",
       " {'label': 'tabby, tabby cat', 'score': 0.03293466567993164}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HF Example\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"image-classification\",\n",
    "    model=\"facebook/dinov2-small-imagenet1k-1-layer\",\n",
    "    dtype=torch.float16,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8def47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc1c43d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(cls_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90459edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.8935735   0.5611535   0.7221834  ... -1.6112721  -1.7305301\n",
      "  -1.24625   ]\n",
      " [ 0.959288    1.5554346  -0.45038044 ...  0.38674653 -1.9512769\n",
      "   0.13411409]\n",
      " [ 0.90353054  4.137877    1.1482705  ... -3.668721    0.68040836\n",
      "   1.5894419 ]\n",
      " ...\n",
      " [ 1.7816428  -0.3815659   0.29235205 ... -0.32355672  0.05422599\n",
      "   0.9596687 ]\n",
      " [-1.0558149   0.5838056  -1.5675911  ... -1.1514442   0.01467256\n",
      "  -1.4091729 ]\n",
      " [ 0.10558449  1.7960936  -1.2238214  ... -0.98181576  2.1069787\n",
      "  -1.3743852 ]]\n",
      "[[48.284294  4.02083 ]\n",
      " [37.122986 -3.568118]\n",
      " [52.24937   8.912947]\n",
      " ...\n",
      " [44.60789  11.12855 ]\n",
      " [50.534054 -3.564082]\n",
      " [50.379196  4.45012 ]]\n"
     ]
    }
   ],
   "source": [
    "file=hdf5.open_HDF5(\"downloaded_embeddings.h5\")\n",
    "vectors=file[\"vectors\"]\n",
    "vectors=vectors[:] #load into a np array\n",
    "labels=file[\"coordinates\"]\n",
    "labels=labels[:]\n",
    "print(vectors)\n",
    "print(labels)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b9a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinates: shape=(3167055, 2), dtype=float32\n",
      "vectors: shape=(3167055, 768), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import hdf5\n",
    "#Size of hp5\n",
    "print(\"agah\")\n",
    "path=\"/home/adam/source/CLIP/full_dataset_embeddings.h5\"\n",
    "hdf5.get_size(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the clip model\n",
    "\n",
    "\n",
    "import utils\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "importlib.reload(utils)\n",
    "from tqdm import tqdm\n",
    "\n",
    "#for having the right model; print the model if unsurea about the layers\n",
    "dim_fourier_encoding=64 #multiple of 4!!\n",
    "dim_hidden=256\n",
    "dim_emb=128 #this one is actually shared with img embeddings\n",
    "device=\"cuda\"\n",
    "data_path=\"/home/adam/source/CLIP/full_dataset_embeddings.h5\"\n",
    "#dim image layer size: \n",
    "#As of now: linear from 768 to dim_emb. We could also have MLP if non-linearity needed\n",
    "\n",
    "image_encoder=nn.Linear(768,dim_emb).to(device)\n",
    "pos_encoder=utils.Fourier_MLP(original_dim=2, fourier_dim=dim_fourier_encoding, hidden_dim=dim_hidden, output_dim=dim_emb).to(device)\n",
    "\n",
    "model= utils.DoubleNetwork(image_encoder,pos_encoder).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/adam/source/CLIP/save_14_to_17-10-25/model.pt\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "nbr_iter=500000\n",
    "mean_sim, mean_asim, std_sim, std_asim = utils.test_similarity(data_path,model, nbr_samples=2,device=\"cuda\",nbr_iter=nbr_iter,plot=True)\n",
    "print(\"Mean:\", mean_sim, mean_asim)\n",
    "print(\"Std:\", std_sim, std_asim)\n",
    "#for the whole test  set:\n",
    "#Mean: tensor(0.1810, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0827, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "#Std: tensor(0.0645, device='cuda:0', grad_fn=<SqrtBackward0>) tensor(0.0726, device='cuda:0', grad_fn=<SqrtBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5215ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gbifID', 'accessRights', 'bibliographicCitation', 'language',\n",
      "       'license', 'modified', 'publisher', 'references', 'rightsHolder',\n",
      "       'type',\n",
      "       ...\n",
      "       'publishedByGbifRegion', 'level0Gid', 'level0Name', 'level1Gid',\n",
      "       'level1Name', 'level2Gid', 'level2Name', 'level3Gid', 'level3Name',\n",
      "       'iucnRedListCategory'],\n",
      "      dtype='object', length=226)\n",
      "Number of datapoints in Switzerland: 74963\n",
      "Number of datapoints in France: 884128\n"
     ]
    }
   ],
   "source": [
    "#Number of occurences in Switwerland\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'data.tsv' with the path to your file\n",
    "file_path = '/home/adam/source/CLIP/data_plantnet_obsevations/occurrence.txt'\n",
    "\n",
    "# Load the TSV file\n",
    "df = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# Check the column names for country info\n",
    "print(df.columns)\n",
    "\n",
    "# Filter rows where the country is Switzerland\n",
    "switzerland_data = df[df['countryCode'] == 'CH']\n",
    "\n",
    "# Count the number of rows\n",
    "num_datapoints = len(switzerland_data)\n",
    "\n",
    "print(f\"Number of datapoints in Switzerland: {num_datapoints}\")\n",
    "\n",
    "# Filter rows where the country is Switzerland\n",
    "france_data = df[df['countryCode'] == 'FR']\n",
    "\n",
    "# Count the number of rows\n",
    "num_datapoints = len(france_data)\n",
    "\n",
    "print(f\"Number of datapoints in France: {num_datapoints}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1855059",
   "metadata": {},
   "source": [
    "densité de points en suisse: \n",
    "74963/41 285 km2=1.81\n",
    "\n",
    "France: 884128/632 702 km2=1.39\n",
    "We good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa06de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countryCode\n",
      "FR    884128\n",
      "ES    253748\n",
      "DE    237009\n",
      "IT    199536\n",
      "GB    132793\n",
      "US    125428\n",
      "NL    104618\n",
      "CH     74963\n",
      "CZ     61113\n",
      "BE     59173\n",
      "AT     44565\n",
      "PL     36438\n",
      "CA     29717\n",
      "PT     28453\n",
      "RU     22644\n",
      "HU     21479\n",
      "SK     16528\n",
      "BR     15615\n",
      "GR     14699\n",
      "KE     14308\n",
      "SE     13236\n",
      "HR     11800\n",
      "TR     11083\n",
      "IN     11004\n",
      "IE      9793\n",
      "DK      9445\n",
      "NO      9042\n",
      "UA      8886\n",
      "RO      8085\n",
      "FI      8053\n",
      "SI      7181\n",
      "RE      6587\n",
      "AR      6290\n",
      "AU      4955\n",
      "BG      4680\n",
      "CL      3733\n",
      "MX      3678\n",
      "RS      3487\n",
      "CR      3089\n",
      "MA      3013\n",
      "DZ      2681\n",
      "CO      2623\n",
      "GP      2530\n",
      "LU      2396\n",
      "ZA      2232\n",
      "EE      2055\n",
      "MQ      1981\n",
      "BA      1895\n",
      "ID      1850\n",
      "JP      1669\n",
      "IL      1645\n",
      "IR      1634\n",
      "BY      1602\n",
      "ME      1554\n",
      "LV      1549\n",
      "LT      1506\n",
      "NC      1490\n",
      "IS      1312\n",
      "TH      1302\n",
      "MG      1248\n",
      "CG      1143\n",
      "NZ      1088\n",
      "CY      1075\n",
      "GE       996\n",
      "MT       969\n",
      "DO       946\n",
      "AL       825\n",
      "PE       783\n",
      "GF       741\n",
      "UY       719\n",
      "MY       712\n",
      "VN       676\n",
      "AD       672\n",
      "CN       668\n",
      "TN       656\n",
      "EC       651\n",
      "PH       624\n",
      "PA       589\n",
      "PR       537\n",
      "VE       499\n",
      "NG       494\n",
      "MU       486\n",
      "TZ       486\n",
      "PF       484\n",
      "PK       445\n",
      "GT       421\n",
      "GG       394\n",
      "MK       385\n",
      "SG       372\n",
      "BO       371\n",
      "LK       367\n",
      "CM       359\n",
      "MD       344\n",
      "AM       319\n",
      "CU       312\n",
      "NP       304\n",
      "JE       300\n",
      "SN       291\n",
      "KZ       286\n",
      "LB       282\n",
      "HN       262\n",
      "CD       253\n",
      "KR       227\n",
      "KH       217\n",
      "SV       207\n",
      "TW       206\n",
      "AZ       204\n",
      "CI       204\n",
      "BD       196\n",
      "LI       190\n",
      "GH       182\n",
      "EG       177\n",
      "ET       174\n",
      "JM       173\n",
      "SY       172\n",
      "UG       170\n",
      "LA       170\n",
      "SA       163\n",
      "OM       161\n",
      "CV       160\n",
      "BJ       142\n",
      "MZ       138\n",
      "UZ       137\n",
      "BL       137\n",
      "NI       137\n",
      "HK       132\n",
      "JO       117\n",
      "PY       115\n",
      "SC       112\n",
      "TT       112\n",
      "KG       109\n",
      "PS        99\n",
      "AX        97\n",
      "GN        97\n",
      "BZ        90\n",
      "BT        89\n",
      "SR        87\n",
      "AE        86\n",
      "BF        84\n",
      "IM        84\n",
      "MS        81\n",
      "GM        81\n",
      "BB        80\n",
      "MF        79\n",
      "XK        78\n",
      "GI        78\n",
      "SM        77\n",
      "VI        77\n",
      "MC        74\n",
      "ST        68\n",
      "RW        68\n",
      "LC        68\n",
      "ZW        64\n",
      "ZM        63\n",
      "PM        61\n",
      "FO        60\n",
      "GA        59\n",
      "YT        58\n",
      "TG        58\n",
      "MV        53\n",
      "BQ        52\n",
      "DM        52\n",
      "WF        51\n",
      "TD        51\n",
      "IQ        51\n",
      "CW        50\n",
      "MM        49\n",
      "LY        48\n",
      "BS        42\n",
      "MR        37\n",
      "HT        36\n",
      "ML        35\n",
      "PG        32\n",
      "MW        28\n",
      "WS        28\n",
      "CK        24\n",
      "BM        23\n",
      "FJ        22\n",
      "GL        22\n",
      "AG        22\n",
      "BN        21\n",
      "MN        20\n",
      "VC        20\n",
      "TC        19\n",
      "GW        19\n",
      "VG        19\n",
      "AO        17\n",
      "BW        17\n",
      "YE        17\n",
      "AW        17\n",
      "KM        17\n",
      "KY        17\n",
      "SD        16\n",
      "GY        16\n",
      "GD        16\n",
      "SJ        15\n",
      "TL        15\n",
      "SX        14\n",
      "VU        13\n",
      "DJ        13\n",
      "SZ        13\n",
      "SS        12\n",
      "CF        12\n",
      "NE        11\n",
      "TJ         9\n",
      "VA         9\n",
      "KN         9\n",
      "SL         8\n",
      "TO         8\n",
      "AI         7\n",
      "MP         6\n",
      "KP         5\n",
      "KW         5\n",
      "SH         4\n",
      "EH         4\n",
      "QA         4\n",
      "GU         4\n",
      "BI         3\n",
      "AF         3\n",
      "MO         2\n",
      "LS         2\n",
      "LR         2\n",
      "BH         2\n",
      "PW         1\n",
      "SB         1\n",
      "ER         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'data.tsv' with the path to your file\n",
    "file_path =  '/home/adam/source/CLIP/data_plantnet_obsevations/occurrence.txt'\n",
    "\n",
    "# Load the TSV file\n",
    "df = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# Count datapoints per country\n",
    "country_counts = df['countryCode'].value_counts()\n",
    "\n",
    "# Display results\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Avoid line wrapping\n",
    "pd.set_option('display.max_colwidth', None)  # Full column width\n",
    "print(country_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024019a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46.95108277187109, 7.43863242087181)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/tmp/ipykernel_1217216/2512154143.py:9: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  lon,lat, _=transform(pCH,pWorld, east,north,0)\n"
     ]
    }
   ],
   "source": [
    "from pyproj import Proj, transform\n",
    "\n",
    "def CH_to_coord(east, north): \n",
    "    '''Mind the order! It swaps to respect both order conventions\n",
    "    ETH doc: https://ia.arch.ethz.ch/lat-lon-to-ch-coordinates/'''\n",
    "\n",
    "    pWorld = Proj(init=\"epsg:4326\")\n",
    "    pCH = Proj(init=\"epsg:2056\")\n",
    "    lon,lat, _=transform(pCH,pWorld, east,north,0)\n",
    "    return lat,lon\n",
    "\n",
    "#Bern: 2,600,000 m (east) and N = 1,200,000 m (north).\n",
    "east=2600000\n",
    "north=1200000\n",
    "e=46\n",
    "n=7\n",
    "result=CH_to_coord(east, north)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b4ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoubleNetwork(\n",
       "  (image_encoder): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (pos_encoder): Fourier_MLP(\n",
       "    (MLP): CustomMLP(\n",
       "      (lin1): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (relu1): ReLU()\n",
       "      (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (relu2): ReLU()\n",
       "      (lin3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the Model\n",
    "import utils\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "importlib.reload(utils)\n",
    "from tqdm import tqdm\n",
    "dim_fourier_encoding=64 #multiple of 4!!\n",
    "dim_hidden=256\n",
    "dim_emb=128 #this one is actually shared with img embeddings\n",
    "device=\"cuda\"\n",
    "data_path=\"/home/adam/source/CLIP/full_dataset_embeddings.h5\"\n",
    "#dim image layer size: \n",
    "#As of now: linear from 768 to dim_emb. We could also have MLP if non-linearity needed\n",
    "\n",
    "image_encoder=nn.Linear(768,dim_emb).to(device)\n",
    "pos_encoder=utils.Fourier_MLP(original_dim=2, fourier_dim=dim_fourier_encoding, hidden_dim=dim_hidden, output_dim=dim_emb).to(device)\n",
    "\n",
    "model= utils.DoubleNetwork(image_encoder,pos_encoder).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/adam/source/CLIP/save_14_to_17-10-25/model.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting embeddings\n",
      "shape Xemb (35105, 128)\n",
      "shape Xcov (35105, 13)\n",
      "shape y (35105, 30)\n",
      "fitting models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 27.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      " 37%|███▋      | 11/30 [00:00<00:00, 89.83it/s]/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      " 80%|████████  | 24/30 [00:00<00:00, 97.35it/s]/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/home/adam/anaconda3/envs/CLIP/lib/python3.13/site-packages/sklearn/linear_model/_glm/glm.py:286: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\n",
      "100%|██████████| 30/30 [00:00<00:00, 81.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, ..., False, False, False],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False,  True],\n",
       "       [False, False, False, ..., False, False,  True],\n",
       "       [False, False, False, ..., False, False,  True]], shape=(35105, 30))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model\n",
    "\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import importlib\n",
    "import torch\n",
    "importlib.reload(utils)\n",
    "\n",
    "pd.set_option('display.max_columns', None)   # show all columns\n",
    "pd.set_option('display.width', None)         # don't wrap lines\n",
    "pd.set_option('display.max_colwidth', None)  # show full column content if needed\n",
    "#np.set_printoptions(threshold=np.inf)      # print entire array, no truncation\n",
    "#np.set_printoptions(linewidth=np.inf)     \n",
    "\n",
    "\n",
    "po_data=pd.read_csv(\"data_SDM_NCEAS/SWItrain_po.csv\")\n",
    "\n",
    "y = pd.get_dummies(po_data[\"spid\"])\n",
    "po_data = pd.concat([po_data.drop(columns=[\"spid\"]), y], axis=1) #For the full one-hot-encoded dataframe\n",
    "\n",
    "po_covariates=po_data.loc[:, [\"bcc\",\"calc\",\"ccc\",\"ddeg\",\"nutri\",\"pday\",\"precyy\",\"sfroyy\",\"slope\",\"sradyy\",\"swb\",\"tavecc\",\"topo\"]]\n",
    "\n",
    "X_cov=po_covariates.to_numpy()\n",
    "y=y.to_numpy()\n",
    "\n",
    "def fit_multi_GLM(X,y):\n",
    "    PR_list=[]\n",
    "    for i in tqdm(range(y.shape[1])):\n",
    "        PR=PoissonRegressor()\n",
    "        PR.fit(X,y[:,i])\n",
    "        PR_list.append(PR)\n",
    "    return PR_list\n",
    "\n",
    "'''def get_embeddings(df,pos_encoder, dim_emb=128,device=\"cuda\"):\n",
    "    X = []\n",
    "    for idx, row in df.iterrows():\n",
    "        lat, lon= utils.CH_to_coord(df[\"y\"],df[\"x\"]) #Swiss epsg:21781 convention on the order\n",
    "        if idx % 1000==0:\n",
    "            print(\"lat\", lat)\n",
    "            print(\"lon\", lon)\n",
    "        coord = torch.tensor([[lat, lon]], dtype=torch.float32).to(device)\n",
    "        embedding = pos_encoder(coord)\n",
    "        X.append(embedding.cpu().detach().numpy())\n",
    "    return X'''\n",
    "\n",
    "from pyproj import Transformer\n",
    "\n",
    "transformer_CH_to_WGS = Transformer.from_crs(\n",
    "    \"EPSG:21781\", \"EPSG:4326\", always_xy=True\n",
    ")\n",
    "\n",
    "def get_embeddings(df, pos_encoder, device=\"cuda\"):\n",
    "    # Vectorized coordinate conversion\n",
    "    lons, lats = transformer_CH_to_WGS.transform(\n",
    "        df[\"x\"].values, df[\"y\"].values\n",
    "    )\n",
    "\n",
    "    coords = torch.tensor(\n",
    "        np.column_stack([lats, lons]),\n",
    "        dtype=torch.float32\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = pos_encoder(coords).cpu().numpy()\n",
    "\n",
    "    return emb\n",
    "\n",
    "\n",
    "\n",
    "print(\"getting embeddings\")\n",
    "X_emb=get_embeddings(po_data, model.pos_encoder)\n",
    "print(\"shape Xemb\", X_emb.shape)\n",
    "print(\"shape Xcov\", X_cov.shape)\n",
    "print(\"shape y\",y.shape)\n",
    "#Fit GLM\n",
    "print(\"fitting models\")\n",
    "\n",
    "PR_emb=fit_multi_GLM(X_emb,y)\n",
    "PR_cov=fit_multi_GLM(X_cov,y)\n",
    "\n",
    "\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f8d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010459797034555213\n"
     ]
    }
   ],
   "source": [
    "#PA data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pa_data=pd.read_csv(\"data_SDM_NCEAS/SWItest_pa.csv\")\n",
    "env=pd.read_csv(\"data_SDM_NCEAS/SWItest_env.csv\")\n",
    "\n",
    "y_target = pa_data.loc[:, [\n",
    "    'swi01','swi02','swi03','swi04','swi05','swi06','swi07','swi08','swi09','swi10',\n",
    "    'swi11','swi12','swi13','swi14','swi15','swi16','swi17','swi18','swi19','swi20',\n",
    "    'swi21','swi22','swi23','swi24','swi25','swi26','swi27','swi28','swi29','swi30'\n",
    "]]\n",
    "X_test = env.loc[:, [\n",
    "    'bcc', 'calc', 'ccc', 'ddeg', 'nutri', 'pday', 'precyy',\n",
    "    'sfroyy', 'slope', 'sradyy', 'swb', 'tavecc', 'topo'\n",
    "]]\n",
    "X_test=X_test.to_numpy()\n",
    "y_target=y_target.to_numpy()\n",
    "\n",
    "y_pred_cov_0=PR_cov[0].predict(X_test)\n",
    "#y_pred_emb_0=PR_emb[0].predict(X_test) Not that simple\n",
    "\n",
    "mse = mean_squared_error(y_target[:,0], y_pred_cov_0)\n",
    "print(\"MSE:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
